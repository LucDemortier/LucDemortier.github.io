I"Âm<p><label for="ADS-0" class="margin-toggle"> ‚äï</label><input type="checkbox" id="ADS-0" class="margin-toggle" /><span class="marginnote"><strong>Contents:</strong> <br /><a href="#AirlineDataSet">1. The airline data set</a> <br /><a href="#CategoricalFeatures">2. Categorical features</a> <br /><a href="#IntrinsicDiscrepancies">3. Intrinsic discrepancies</a> <br /><a href="#FittingStrategy">4. Fitting Strategy</a> <br /><a href="#GridSearch">5. Grid search</a> <br /><a href="#ClassifierPerformance">6. Classifier performance</a> <br /><a href="#TechnicalNote">7. Technical note</a> </span>
On Sunday June 5 of this year, the NYC meetup group <a href="https://www.meetup.com/NYC-Women-in-Machine-Learning-Data-Science/">Women in Machine Learning and Data Science</a> organized a <a href="https://www.meetup.com/NYC-Women-in-Machine-Learning-Data-Science/events/230658399/">workshop on random forests</a> led by Anita Schmid and Christine Hurtubise from <a href="https://www.ondeck.com/">OnDeck</a>. They started with a theoretical review of the random forest algorithm, followed with some practical tips, and after that we all broke up into small groups to try our hands on a data set of flight delays. Some of us used Python, others R.</p>

<p>There wasn‚Äôt really enough time to complete the analysis during the workshop, so I finished it at home and report the results here.</p>

<p><label for="btt-0" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-0" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="AirlineDataSet"></a></p>

<h2 id="the-airline-data-set">The airline data set</h2>
<p>The data set consists of 100,000 flight records (this is actually a subsample of a large <a href="https://stat-computing.org/dataexpo/2009/the-data.html">public data set</a>).  After some basic cleanup<label for="ADS-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ADS-1" class="margin-toggle" /><span class="sidenote">The cleanup consisted in converting the ‚ÄúMonth‚Äù, ‚ÄúDayofMonth‚Äù, and ‚ÄúDayOfWeek‚Äù fields from string to integer, and mapping ‚ÄúDepTime‚Äù values larger than 2400 back to the interval [0,2400]. </span>, each record contains the following fields:</p>

<p><label for="TB-1" class="margin-toggle"> ‚äï</label><input type="checkbox" id="TB-1" class="margin-toggle" /><span class="marginnote">Table 1: Features of the airline data set; the last row (dep_delayed_15min) is the response variable; the type column indicates whether a feature is binary (bin), integer (int), or categorical (cat). </span></p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">¬†</th>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Description</th>
      <th style="text-align: left">Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">1.</td>
      <td style="text-align: left">Year</td>
      <td style="text-align: left">A number between 2004 and 2007</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">2.</td>
      <td style="text-align: left">Month</td>
      <td style="text-align: left">A number between 1 and 12</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">3.</td>
      <td style="text-align: left">DayofMonth</td>
      <td style="text-align: left">A number between 1 and 31</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">4.</td>
      <td style="text-align: left">DayOfWeek</td>
      <td style="text-align: left">A number between 1 and 7</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">5.</td>
      <td style="text-align: left">DepTime</td>
      <td style="text-align: left">A number between 0 and 2400</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">6.</td>
      <td style="text-align: left">UniqueCarrier</td>
      <td style="text-align: left">Two-character airline code</td>
      <td style="text-align: left">cat</td>
    </tr>
    <tr>
      <td style="text-align: right">7.</td>
      <td style="text-align: left">Origin</td>
      <td style="text-align: left">Three-letter departure airport code</td>
      <td style="text-align: left">cat</td>
    </tr>
    <tr>
      <td style="text-align: right">8.</td>
      <td style="text-align: left">Dest</td>
      <td style="text-align: left">Three-letter destination airport code</td>
      <td style="text-align: left">cat</td>
    </tr>
    <tr>
      <td style="text-align: right">9.</td>
      <td style="text-align: left">Distance</td>
      <td style="text-align: left">Flight distance in miles</td>
      <td style="text-align: left">int</td>
    </tr>
    <tr>
      <td style="text-align: right">10.</td>
      <td style="text-align: left">dep_delayed_15min</td>
      <td style="text-align: left">Y/N flag indicating a departure</td>
      <td style="text-align: left">¬†</td>
    </tr>
    <tr>
      <td style="text-align: right">¬†</td>
      <td style="text-align: left">¬†</td>
      <td style="text-align: left">delay of more than 15 minutes</td>
      <td style="text-align: left">bin</td>
    </tr>
  </tbody>
</table>

<p>The purpose of the exercise is to construct a predictor of the binary delay variable (‚Äúdep_delay_15min‚Äù) based on the nine available features. The input data set is unbalanced: 80,726 flights out of 100,000 had no delay.</p>

<p>To gain some insight into the features it is helpful to plot their normalized distributions separately for the ‚Äúdelay‚Äù and ‚Äúno-delay‚Äù cases. This is shown below in Figures 1, 2, and 3.  Because of their normalization, these plots have a simple statistical interpretation: at any point along the X-axis, the ratio of the two distributions is a likelihood ratio. The top-left panel of Figure 1 for example, indicates that the likelihood of a delay increased from 2004 to 2007.</p>

<figure class="fullwidth"><img src="/assets/img/blog/007_RandomForests/FlightDelayFeatures1.png" /><figcaption>Figure 1: Histograms of the year, month, day of month, day of week, departure time, and flight distance of the flights in the airline data set, for flights with (orange) and without (blue) delay. All distributions are normalized to unit area in order to emphasize differences in shape.</figcaption></figure>

<p>Several other effects are visible in these plots: delays tend to be more likely during summer months and in December, in the second half of any given month, in the middle of the week, and at the end of the day. Short flights seem less likely to be delayed than long ones. Perhaps the sharpest difference between delay and no-delay is exhibited by the histogram of departure times.</p>

<p>The remaining features are categorical, and their distributions can be represented with bar charts.  First the airline codes, ordered alphabetically. There are 23 of them:</p>

<figure><figcaption>Figure 2: Bar chart of airline codes for flights with (orange) and without (blue) delay. The charts are normalized to the same area.</figcaption><img src="/assets/img/blog/007_RandomForests/FlightDelayFeatures2.png" /></figure>

<p>Next, the airports of departure and destination. There are 307 of the former and 306 of the latter, so these are long charts. For readability I have chosen to display them vertically, and ordered alphabetically. The main thing one learns from these charts is that some airports are busier than others, and busyness correlates with delays (see for example ATL-Atlanta, EWR-Newark, and ORD-Chicago), but not in a systematic way (DFW-Dallas/Fort Worth, IAH-Houston, and LAX-Los Angeles). Enjoy the scroll down, and I‚Äôll see you on the other side of YUM (Yuma, AZ)‚Ä¶</p>

<figure><figcaption>Figure 3: Bar charts of departure (left) and destination (right) airports for flights with (orange) and without (blue) delay. The charts are normalized to the same area.</figcaption><img src="/assets/img/blog/007_RandomForests/FlightDelayFeatures3a.png" /></figure>

<p><label for="btt-1" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-1" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="CategoricalFeatures"></a></p>

<h2 id="categorical-features">Categorical features</h2>
<p>To prepare the data for the classifier, the categorical features (‚ÄúUniqueCarrier‚Äù, ‚ÄúOrigin‚Äù, and ‚ÄúDest‚Äù, see Table 1) must be converted to dummy variables using one-hot encoding. Pandas has an easy method for this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df2 = pd.get_dummies(df1, drop_first=False)
</code></pre></div></div>

<p>This results in data frame <code class="language-plaintext highlighter-rouge">df1</code>, with 9 feature variables, being replaced by data frame <code class="language-plaintext highlighter-rouge">df2</code>, which has 642 feature variables! The <code class="language-plaintext highlighter-rouge">drop_first=False</code> argument indicates that we do not wish to drop one dummy variable per categorical variable<label for="ADS-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ADS-2" class="margin-toggle" /><span class="sidenote">See for example Galit Shmueli, <a href="http://www.bzst.com/2014/03/the-use-of-dummy-variables-in.html">‚ÄúThe use of dummy variables in predictive algorithms‚Äù</a>. </span>. This is only necessary for regression problems, where input multicollinearity is an issue (‚Äúdummy variable trap‚Äù). For tree-based classification models, removing a dummy variable may actually reduce efficiency.</p>

<p><label for="btt-2" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-2" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="IntrinsicDiscrepancies"></a></p>

<h2 id="intrinsic-discrepancies">Intrinsic discrepancies</h2>
<p>For future reference I find it useful to summarize with one number the effectiveness of a feature to distinguish between delay and no-delay. A relatively simple, information-based measure to achieve this is the so-called intrinsic discrepancy<label for="ADS-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ADS-3" class="margin-toggle" /><span class="sidenote">See for example J. Bernardo, ‚Äú<a href="https://www.uv.es/~bernardo/RefAna.pdf">Reference Analysis</a>‚Äù, Handbook of Statistics 25 (D. K. Dey and C. R. Rao, eds.), 17-90, Elsevier, 2005. </span> between two probability distributions <span>‚Äã<script type="math/tex">p_{1}</script></span> and <span>‚Äã<script type="math/tex">p_{2}</script></span>:</p>
<div class="mathblock"><script type="math/tex; mode=display"> \delta\{p_{1},p_{2}\}\;=\;\min\Big\{ \int p_{1}(x)\,\log\frac{p_{1}(x
)}{p_{2}(x)}\,dx,\; \int p_{2}(x)\,\log\frac{p_{2}(x)}{p_{1}(x)}\,dx \Big\}. </script></div>
<p>For discrete distributions the integrals become sums.  After sorting them in decreasing order, the top ten intrinsic discrepancies between delay and no-delay are as follows:</p>

<p><label for="TB-2" class="margin-toggle"> ‚äï</label><input type="checkbox" id="TB-2" class="margin-toggle" /><span class="marginnote">Table 2: Intrinsic discrepancies between the delay and no-delay distributions of the features in the airline data set. The features ‚ÄúOrigin_ORD‚Äù, ‚ÄúUniqueCarrier_HA‚Äù, ‚ÄúUniqueCarrier_EV‚Äù, and ‚ÄúOrigin_HNL‚Äù are some of the dummy variables created by one-hot encoding of the categorical features in the original data set. </span></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: center">Intrinsic Discrepancy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">DepTime</td>
      <td style="text-align: center">0.2400</td>
    </tr>
    <tr>
      <td style="text-align: left">Month</td>
      <td style="text-align: center">0.0205</td>
    </tr>
    <tr>
      <td style="text-align: left">Distance</td>
      <td style="text-align: center">0.0114</td>
    </tr>
    <tr>
      <td style="text-align: left">DayOfWeek</td>
      <td style="text-align: center">0.0069</td>
    </tr>
    <tr>
      <td style="text-align: left">Year</td>
      <td style="text-align: center">0.0063</td>
    </tr>
    <tr>
      <td style="text-align: left">Origin_ORD</td>
      <td style="text-align: center">0.0058</td>
    </tr>
    <tr>
      <td style="text-align: left">UniqueCarrier_HA</td>
      <td style="text-align: center">0.0056</td>
    </tr>
    <tr>
      <td style="text-align: left">DayofMonth</td>
      <td style="text-align: center">0.0050</td>
    </tr>
    <tr>
      <td style="text-align: left">UniqueCarrier_EV</td>
      <td style="text-align: center">0.0046</td>
    </tr>
    <tr>
      <td style="text-align: left">Origin_HNL</td>
      <td style="text-align: center">0.0036</td>
    </tr>
  </tbody>
</table>

<p>It is again clear that departure time is by far the most discriminating feature, followed by month and distance. The Chicago airport (ORD) as point of origin is a noteworthy feature by itself, as are Hawaiian Airlines (HA), Express Jet (EV), and Honolulu airport (HNL).</p>

<p><label for="btt-3" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-3" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="FittingStrategy"></a></p>

<h2 id="fitting-strategy">Fitting strategy</h2>
<p>When fitting a classifier to training data, an important concern is to avoid <em>overfitting</em>, in order to keep the <a href="https://en.wikipedia.org/wiki/Generalization_error">generalization error</a> under control.  This is a measure of the accuracy of the classifier when applied to previously unseen data.</p>

<p>This concern leads to the distinction between parameters and hyper-parameters.  Parameters are quantities that are adjusted to fit a classifier to training data, whereas hyper-parameters are used to control overfitting. In the case of a random forest for example, parameters specify what happens at each node of each decision tree, whereas hyper-parameters specify the number of trees and the maximum depth of a tree.</p>

<p>A common strategy to determine the hyper-parameters is to maximize the classifier accuracy, defined as the fraction of cases that are correctly classified. However this is not a good strategy for unbalanced classes, as is the case in the airline data set, where we have 19% ‚Äúpositives‚Äù (delayed flights) versus 81% ‚Äúnegatives‚Äù (non-delayed flights). To see this, note that accuracy can be written as:</p>
<div class="mathblock"><script type="math/tex; mode=display">
Accuracy = p\times Recall \;+\; (1-p)\times Specificity,
</script></div>
<p>where <span>‚Äã<script type="math/tex">p</script></span> is the prevalence, the fraction of positives in the sample. Since the recall (the fraction of positives that are correctly classified) and specificity (the fraction of negatives that are correctly classified) are independent of <span>‚Äã<script type="math/tex">p</script></span>, the above equation shows that accuracy does depend on <span>‚Äã<script type="math/tex">p</script></span> and is therefore not an intrinsic property of the classifier.</p>

<p>A better measure to maximize is AUC, the area under the Receiver Operating Characteristic<label for="ADS-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ADS-4" class="margin-toggle" /><span class="sidenote">See for example Tom Fawcett, ‚Äú<a href="https://www.sciencedirect.com/science/article/pii/S016786550500303X">An Introduction to ROC Analysis</a>‚Äù, Pattern Recognition Letters 27, no. 8 (June 2006), 861‚Äì874. </span> (ROC). It is independent of the prevalence and can be interpreted as the probability for the classifier to produce a higher score for a randomly drawn positive than for a randomly drawn negative.</p>

<p>A good fitting strategy, then, starts with a three-fold cross-validated search over a grid of hyper-parameters, and follows with testing on a holdout sample. Here are the steps:</p>

<ol>
  <li>
    <p>Randomly split the entire data set into a training set and a test set, in the ratio of 70% to 30%.</p>
  </li>
  <li>
    <p>Specify a ‚Äúreasonable‚Äù search grid in hyper-parameter space.</p>
  </li>
  <li>
    <p>Using the training set, perform a three-fold cross validation fit at each point of the search grid. Use AUC to determine the optimal point.</p>
  </li>
  <li>
    <p>Refit the random forest to the entire training set, using the hyper-parameter values at the optimal point from the grid search.</p>
  </li>
  <li>
    <p>Evaluate the properties of the fitted classifier on the test set.</p>
  </li>
</ol>

<p>In the next sections I describe the grid search and the result of applying the refitted classifier from step 4 to the test set.</p>

<p><label for="btt-4" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-4" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="GridSearch"></a></p>

<h2 id="grid-search">Grid search</h2>
<p><a href="https://scikit-learn.org/stable/">Scikit-learn</a> provides method <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code class="language-plaintext highlighter-rouge">GridSearchCV</code></a> to search a grid in hyper-parameter space for the optimal classifier of a given type in a given problem. In the present case we are interested in a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><code class="language-plaintext highlighter-rouge">RandomForestClassifier</code></a>. As was emphasized at the WiMLDS workshop, random forests can and do sometimes overfit. Scikit-learn provides three parameters to control overfitting, and these are the ones I used in the grid search:</p>

<ul>
  <li>
    <p><strong>n_estimators:</strong> the number of trees in the forest; more trees reduces overfitting but takes longer to run; I tried the values 50, 100, 200.</p>
  </li>
  <li>
    <p><strong>max_depth:</strong> the maximum depth of a tree in the forest; the deeper a tree, the more it risks overfitting; I tried the values 5, 10, 20.</p>
  </li>
  <li>
    <p><strong>min_samples_leaf:</strong> the minimum number of training examples in a newly created leaf in a tree; the smaller this number, the higher the risk of overfit; I tried the values 1, 2, 3, 4, 5, 10, 20.</p>
  </li>
</ul>

<p>The remaining random forest parameters were set to their scikit-learn default values, except for <strong>class_weight</strong>, which was set to ‚Äúbalanced_subsample‚Äù.  Among the grid search parameters, <strong>scoring</strong> was set to ‚Äúroc_auc‚Äù, which is the AUC defined in the previous section.</p>

<p>The result of the grid search is <strong>n_estimators = 200</strong>, <strong>max_depth = 20</strong>, and <strong>min_samples_leaf = 2</strong>, which yields an AUC of <span>‚Äã<script type="math/tex">0.711\pm 0.007</script></span> (as obtained by the 3-fold cross validation). There is relatively little variation across the grid, the lowest point having <span>‚Äã<script type="math/tex">{\rm AUC} = 0.681\pm 0.016</script></span>, a 4% difference.</p>

<p><label for="btt-5" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-5" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="ClassifierPerformance"></a></p>

<h2 id="classifier-performance">Classifier performance</h2>
<p>The grid search was performed on 70% of the total sample. I used the remaining 30% as a test sample to compute the confusion matrix and make summary plots. Here then is the confusion matrix, where I assigned the label ‚Äúpositive‚Äù to ‚Äúflight delay‚Äù (since that‚Äôs the effect of interest) and the label ‚Äúnegative‚Äù to ‚Äúno delay‚Äù:</p>

<p><label for="TB-3" class="margin-toggle"> ‚äï</label><input type="checkbox" id="TB-3" class="margin-toggle" /><span class="marginnote">Table 3: Confusion matrix for the random forest classifier found with the grid search, showing the number of true negatives (tn), false positives (fp), false negatives (fn), and true positives (tp). A 50% threshold on the classifier output is assumed. </span></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">¬†</th>
      <th style="text-align: right">Predicted No-Delay</th>
      <th style="text-align: right">Predicted Delay</th>
      <th>¬†</th>
      <th style="text-align: right">Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Actual No-Delay</td>
      <td style="text-align: right">tn = 15458</td>
      <td style="text-align: right">fp = 8747</td>
      <td>¬†</td>
      <td style="text-align: right">24205</td>
    </tr>
    <tr>
      <td style="text-align: left">Actual Delay</td>
      <td style="text-align: right">fn = 1881</td>
      <td style="text-align: right">tp = 3914</td>
      <td>¬†</td>
      <td style="text-align: right">5795</td>
    </tr>
    <tr>
      <td style="text-align: left">Total</td>
      <td style="text-align: right">17339</td>
      <td style="text-align: right">12661</td>
      <td>¬†</td>
      <td style="text-align: right">30000</td>
    </tr>
  </tbody>
</table>

<p>From this matrix we can estimate some of the standard quantities that are used to evaluate the performance of a classifier:</p>

<p><label for="TB-4" class="margin-toggle"> ‚äï</label><input type="checkbox" id="TB-4" class="margin-toggle" /><span class="marginnote">Table 4: Performance indicators for the random forest classifier used to predict flight delays. ‚ÄúSensitivity‚Äù is also known as ‚Äúrecall‚Äù, and the two types of precision are sometimes referred to as positive and negative predictive values. </span></p>

<table>
  <tbody>
    <tr>
      <td>Sensitivity</td>
      <td>(probability to identify a true positive):</td>
      <td>67.5%</td>
    </tr>
    <tr>
      <td>Specificity</td>
      <td>(probability to identify a true negative):</td>
      <td>63.9%</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>(probability that a positive id is true):</td>
      <td>30.9%</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>(probability that a negative id is true):</td>
      <td>89.2%</td>
    </tr>
    <tr>
      <td>Accuracy:</td>
      <td>(probability of a correct identification):</td>
      <td>64.6%</td>
    </tr>
  </tbody>
</table>

<p>All the above numbers are relative to a 50% threshold on the output of the classifier.  Here is how this output is distributed, separately for delayed and non-delayed flights, in the training and test samples:</p>

<figure><figcaption>Figure 4: Histograms of the classifier output for delayed flights (orange) and non-delayed flights (blue), in the training subsample (left) and the testing subsample (right).</figcaption><img src="/assets/img/blog/007_RandomForests/RF_probabilities.png" /></figure>

<p>The histograms are centered around 50% because I set the parameter ‚Äúclass_weight‚Äù to ‚Äúbalanced_subsample‚Äù in scikit-learn‚Äôs random forest classifier. Had I not done so, the histograms would have been centered around the fraction of positives in the dataset, which is about 20%, and all bins above 50% would have been empty. As a result the precision and sensitivity would have been zero, and the specificity 100%, due to the 50% threshold on the classifier output.</p>

<p>For predicting flight delays, thresholds other than 50% can be chosen from the left panel of Figure 5. The queue rate is the fraction of instances that pass the threshold cut. As the threshold increases, fewer true positives pass the cut, leading to a decrease in recall that follows the decrease in queue rate. On the other hand the purity of the sample of instances passing the cut increases, as shown by the precision curve.</p>

<figure><figcaption>Figure 5: Precision, recall, and queue rate for flight delay predictions, as a function of the threshold on the classifier output. Left: Result obtained from the test sample. Right: Medians and 90% confidence belts obtained from 50 random splittings of the original data set.</figcaption><img src="/assets/img/blog/007_RandomForests/PRQ2_belts.png" /></figure>

<p>Note how the precision curve in the left panel becomes more jagged for thresholds above 0.60. This is due to a lack of statistics, as can be inferred from the right panel of Figure 4. To explore the variability of the precision, recall, and queue-rate curves as a function of threshold, I generated independent random forest models from 50 different random splittings of the original data set into training and test subsets (still maintaining a 70/30 ratio). I used this ensemble of models to compute and draw median curves and 90% confidence belts, which are shown in Figure 5‚Äôs right panel.</p>

<p>The confidence belts are generally quite narrow, the only exception being the precision curve for thresholds above 0.60. This type of plot can be used to guide business decisions<label for="ADS-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="ADS-5" class="margin-toggle" /><span class="sidenote">For a nice introduction to thresholding, see this blog post by Slater Stich, <a href="https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415">‚ÄúVisualizing Machine Learning Thresholds to Make Better Business Decisions‚Äù</a>. </span>.</p>

<p>As mentioned earlier, our random forest classifier uses 642 features (after converting categorical features to binary ones). It is therefore interesting to check which ones are important, and how much impact important features have compared to the other ones. Scikit-learn provides a RandomForestClassifier attribute to extract feature importances. Each feature importance measures the decrease in classification accuracy when the corresponding feature values are randomly permuted over all trees in the forest.</p>

<figure><figcaption>Figure 6: Bar chart of feature importances, as obtained from scikit-learn's random forest classifier.</figcaption><img src="/assets/img/blog/007_RandomForests/FeatureImportances.png" /></figure>

<p><label for="Fig7" class="margin-toggle">‚äï</label><input type="checkbox" id="Fig7" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/assets/img/blog/007_RandomForests/ROC.png" /><br />Figure 7: Receiver Operating Characteristic for the random forest classifier used to predict flight delays. The curve is shown both for the training data set (orange) and the testing data set (blue).</span>Figure 6 shows that departure time is by far the most important feature, in agreement with the intrinsic discrepancy calculation shown earlier.</p>

<p>The last plot I‚Äôd like to show is that of the Receiver Operating Characteristic (ROC). This is a plot of the true positive rate versus the false positive rate, and is shown in Figure 7. The orange curve is the ROC calculated on the data set on which the random forest was trained, whereas the blue curve was obtained from the testing data set. The area under the orange ROC is 0.786, that under the blue ROC 0.717. The fact that both curves are not too far from each other indicates that there is little overfitting.</p>

<p><label for="btt-6" class="margin-toggle"> ‚äï</label><input type="checkbox" id="btt-6" class="margin-toggle" /><span class="marginnote"><a href="#TopOfPage">Back to Top</a> </span></p>

<p><a name="TechnicalNote"></a></p>

<h2 id="technical-note">Technical note</h2>
<p>The Jupyter notebook for this analysis can be found in <a href="https://github.com/LucDemortier/RandomForests">my GitHub repository RandomForests</a>.</p>
:ET