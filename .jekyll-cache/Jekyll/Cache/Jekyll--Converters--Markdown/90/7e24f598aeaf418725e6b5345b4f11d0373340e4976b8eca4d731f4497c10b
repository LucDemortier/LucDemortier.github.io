I"Z˜<p>[Last updated on 2 September 2024]</p>

<blockquote>
  <p><strong>Contents</strong>
<br /><a href="#Introduction">1. Introduction</a>
<br /><a href="#FourApplications">2. Bayes‚Äô theorem</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#Notation">2.1 Notation</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#Prior">2.2 The prior</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#Likelihood">2.3 The likelihood</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#ModelEvidence">2.4 The model evidence</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#Posterior">2.5 The posterior</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#MeasurementPrecision">2.6 Classifier precision versus measurement precision</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#Summary">2.7 Summary</a>
<br /><a href="#JointProbabilities">3. Joint probabilities</a>
<br /><a href="#LikelihoodRatios">4. Likelihood ratios</a>
<br /><a href="#ClassifierUsefulness">5. When is a classifier useful?</a>
<br /><a href="#ScoreBasedMetrics">6. Classifier scores</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#ROC">6.1 The receiver operating characteristic</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#PRC">6.2 The precision-recall curve</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#ScoresAndUsefulnessCondition">6.3 Classifier Scores and the Usefulness Conditions</a>
<br /><a href="#OperatingPoints">7. Classifier operating points</a>
<br /><a href="#MetricsEstimation">8. Performance metric estimation</a>
<br /><a href="#TrendsAndBaselines">9. Trends and baselines</a>
<br /><a href="#EffectOfPrevalence">10. Effect of prevalence on classifier training and testing</a>
<br /><a href="#Appendix">11. Mathematical Appendix</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#ExpectationValues">11.1 Expectation values of sensitivity, specificity, and accuracy</a>
<br /> ‚ÄÇ‚ÄÇ‚ÄÇ<a href="#MetricsVsThreshold">11.2 Performance metrics versus threshold</a></p>
</blockquote>

<hr />

<p><a name="Introduction"></a></p>
<h2 id="1-introduction">1. Introduction</h2>
<p>I always thought that the <em>confusion</em> matrix was rather aptly named, a reference not so much to the mixed performance of a classifier as to my own bewilderment at the number of measures of that performance. Recently however, I encountered <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">a brief mention</a> of the possibility of a Bayesian interpretation of performance measures, and this inspired me to explore the idea a little further. It‚Äôs not that Bayes‚Äô theorem is needed to understand or apply the performance measures, but it acts as an organizing and connecting principle, which I find helpful. New concepts are easier to remember if they fit inside a good story.</p>

<p>Another advantage of taking the Bayesian route is that this forces us to view performance measures as probabilities, which are <em>estimated</em> from the confusion matrix. Elementary presentations tend to <em>define</em> performance metrics in terms of ratios of confusion matrix elements, thereby ignoring the effect of statistical fluctuations.</p>

<p>Bayes‚Äô theorem is not the only way to generate performance metrics. One can also start from joint probabilities, likelihood ratios, or classifier scores. The next five sections describe these approaches one by one, and include a discussion of a minimal condition for a classifier to be useful. This is then followed by a section on estimating performance measures, one on the effect of changing the prevalence in the training and testing data sets of a classifier, and a technical appendix.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="FourApplications"></a></p>
<h2 id="2-bayes-theorem">2. Bayes‚Äô theorem</h2>
<p>Let‚Äôs start from Bayes‚Äô theorem in its general form:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(\lambda\mid X) \;=\; \frac{p(X\mid\lambda)\;\pi(\lambda)}{\int p(X\mid\lambda^{\prime})\;\pi(\lambda^{\prime})\;d\lambda^{\prime}}.
</script></div>
<p>Here <span>‚Äã<script type="math/tex">X</script></span> represents the observed data and <span>‚Äã<script type="math/tex">\lambda</script></span> a parameter of interest. On the left-hand side is the posterior probability density of <span>‚Äã<script type="math/tex">\lambda</script></span> given <span>‚Äã<script type="math/tex">X</script></span>. On the right-hand side, <span>‚Äã<script type="math/tex">p(X\mid\lambda)</script></span> is the likelihood, or conditional probability density of <span>‚Äã<script type="math/tex">X</script></span> given <span>‚Äã<script type="math/tex">\lambda</script></span>, and <span>‚Äã<script type="math/tex">\pi(\lambda)</script></span> is the prior probability density of <span>‚Äã<script type="math/tex">\lambda</script></span>. The denominator is called marginal likelihood or model evidence. One way to think about Bayes‚Äô theorem is that it uses the data <span>‚Äã<script type="math/tex">X</script></span> to update the prior information <span>‚Äã<script type="math/tex">\pi(\lambda)</script></span> about <span>‚Äã<script type="math/tex">\lambda</script></span>, and returns the posterior <span>‚Äã<script type="math/tex">p(\lambda\mid X)</script></span>.</p>

<p>For a binary classifier <span>‚Äã<script type="math/tex">\lambda</script></span> is the true class to which instance <span>‚Äã<script type="math/tex">X</script></span> belongs and can take only two values, say 0 and 1. The denominator in Bayes‚Äô theorem then simplifies to:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(X) \;\equiv\; \int p(X\mid\lambda^{\prime})\;\pi(\lambda^{\prime})\;d\lambda^{\prime}
     \;=\; p(X\mid\lambda\!=\!0)\,\pi(\lambda\!=\!0)\;+\;
           p(X\mid\lambda\!=\!1)\,\pi(\lambda\!=\!1).
</script></div>

<p>We‚Äôll take a look at each component of Bayes‚Äô theorem in turn: the prior, the likelihood, the model evidence, and finally the posterior. Each of these maps to a classifier performance measure or a population characteristic. But first, a word about notation.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="Notation"></a></p>
<h3 id="21-notation">2.1 Notation</h3>
<p>If our sole purpose is to describe the performance of a classifier in general terms, the data <span>‚Äã<script type="math/tex">X</script></span> can be replaced by the <em>class label</em> <span>‚Äã<script type="math/tex">\ell</script></span> or by the <em>score</em> <span>‚Äã<script type="math/tex">q</script></span> assigned by the classifier (this score is not necessarily a probability and not necessarily between 0 and 1). Thus for example, <span>‚Äã<script type="math/tex"> p(\lambda\!=\!1 \mid \ell\!=\!0) </script></span> represents the posterior probability that the true class is 1 given a class label of 0, and <span>‚Äã<script type="math/tex">p(\lambda\!=\!1\mid q\!\ge\! q_{T})</script></span> is the posterior probability that the true class is 1 given that the score <span>‚Äã<script type="math/tex">q</script></span> is above a pre-specified threshold <span>‚Äã<script type="math/tex">q_{T}</script></span>.</p>

<p>To simplify the notation we‚Äôll write <span>‚Äã<script type="math/tex">\lambda_{0}</script></span> to mean <span>‚Äã<script type="math/tex">\lambda=0</script></span>, <span>‚Äã<script type="math/tex">\lambda_{1}</script></span> to mean <span>‚Äã<script type="math/tex">\lambda=1</script></span>, and similarly with <span>‚Äã<script type="math/tex">\ell_{0}</script></span> and <span>‚Äã<script type="math/tex">\ell_{1}</script></span>. Going one step further, where convenient we‚Äôll write the prior as <span>‚Äã<script type="math/tex">\pi_{i}</script></span> instead of <span>‚Äã<script type="math/tex">\pi(\lambda_{i})</script></span> and the queue rate as <span>‚Äã<script type="math/tex">p_{i}</script></span> instead of <span>‚Äã<script type="math/tex">p(\ell_{i})</script></span>, where <span>‚Äã<script type="math/tex">i=0,1</script></span>. With this notation the equation for the denominator of Bayes‚Äô theorem becomes:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(X) \;=\; p(X\mid\lambda_{0})\,\pi_{0} \;+\; p(X\mid\lambda_{1})\,\pi_{1}.
</script></div>
<p>Finally, we‚Äôll use the notation <span>‚Äã<script type="math/tex">A\;</script></span>&amp;<span>‚Äã<script type="math/tex">\;B</script></span> to indicate the logical <em>and</em> of <span>‚Äã<script type="math/tex">A</script></span> and <span>‚Äã<script type="math/tex">B</script></span>, the notation <span>‚Äã<script type="math/tex">A \lor B</script></span> for its logical <em>or</em>, and <span>‚Äã<script type="math/tex">A \mid B</script></span> for the conditional <span>‚Äã<script type="math/tex">A</script></span> <em>given</em> <span>‚Äã<script type="math/tex">B</script></span>.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="Prior"></a></p>
<h3 id="22-the-prior">2.2 The prior</h3>
<p>The first ingredient in the computation of the posterior is the prior <span>‚Äã<script type="math/tex">\pi(\lambda)</script></span>. To fix ideas, let‚Äôs assume that <span>‚Äã<script type="math/tex">\lambda_{1}</script></span> is the class of interest, generically labeled ‚Äúpositive‚Äù; it indicates ‚Äúeffect‚Äù, ‚Äúsignal‚Äù, ‚Äúdisease‚Äù, ‚Äúfraud‚Äù, or ‚Äúspam‚Äù, depending on the context. Then <span>‚Äã<script type="math/tex">\lambda_{0}</script></span> indicates the lack of these things and is generically labeled ‚Äúnegative‚Äù. To fully specify the prior we just need <span>‚Äã<script type="math/tex">\pi_{1}</script></span>, since <span>‚Äã<script type="math/tex">\pi_{0} = 1 - \pi_{1}</script></span>. The prior probability <span>‚Äã<script type="math/tex">\pi_{1}</script></span> of drawing a positive instance from the population is called the <strong>prevalence</strong>. It is a property of the population, not the classifier.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="Likelihood"></a></p>
<h3 id="23-the-likelihood">2.3 The likelihood</h3>
<p>The next ingredient is the conditional probability <span>‚Äã<script type="math/tex">p(X\mid\lambda)</script></span>. Working with the classifier label <span>‚Äã<script type="math/tex">\ell</script></span> instead of <span>‚Äã<script type="math/tex">X</script></span> leads to the following four combinations:</p>

<ul>
  <li>
    <p>The <strong>true positive rate</strong>, also known as <strong>sensitivity</strong> or <strong>recall</strong>: <span>‚Äã<script type="math/tex">S_{e} \equiv p(\ell_{1}\mid \lambda_{1})</script></span>,</p>
  </li>
  <li>
    <p>The <strong>true negative rate</strong>, also known as <strong>specificity</strong>: <span>‚Äã<script type="math/tex">S_{p}\equiv p(\ell_{0}\mid \lambda_{0})</script></span>,</p>
  </li>
  <li>
    <p>The <strong>false-positive or Type-I-error rate</strong>: <span>‚Äã<script type="math/tex">\alpha\equiv p(\ell_{1}\mid\lambda_{0}) = 1 - S_{p}</script></span>, and</p>
  </li>
  <li>
    <p>The <strong>false-negative or Type-II-error rate</strong>: <span>‚Äã<script type="math/tex">\beta\equiv p(\ell_{0}\mid\lambda_{1}) = 1 - S_{e}</script></span>.</p>
  </li>
</ul>

<p>For example, <span>‚Äã<script type="math/tex">p(\ell_{1}\mid \lambda_{0})</script></span> is the conditional probability that the classifier assigns a label of 1 to an instance actually belonging to class 0. Note that the above four rates are independent of the prevalence and are therefore intrinsic properties of the classifier.</p>

<p>When viewed as a function of true class <span>‚Äã<script type="math/tex">\lambda</script></span>, for fixed label <span>‚Äã<script type="math/tex">\ell</script></span>, the conditional probability <span>‚Äã<script type="math/tex">p(\ell\mid\lambda)</script></span> is known as the <strong>likelihood function</strong>. This functional aspect of <span>‚Äã<script type="math/tex">p(\ell\mid\lambda)</script></span> is not really used here, but it is good to remember the terminology.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="ModelEvidence"></a></p>
<h3 id="24-the-model-evidence">2.4 The model evidence</h3>
<p>Finally, we need the normalization factor in Bayes‚Äô theorem, the model evidence. For a binary classifier this has two components, corresponding to the two possible labels:</p>

<ul>
  <li>
    <p>The <strong>positive labeling rate</strong>: <span>‚Äã<script type="math/tex">p_{1} = p(\ell_{1}\mid\lambda_{0})\,\pi_{0} + p(\ell_{1}\mid\lambda_{1})\,\pi_{1}</script></span>, and</p>
  </li>
  <li>
    <p>The <strong>negative labeling rate</strong>: <span>‚Äã<script type="math/tex">p_{0} = p(\ell_{0}\mid\lambda_{0})\,\pi_{0} + p(\ell_{0}\mid\lambda_{1})\,\pi_{1}</script></span>.</p>
  </li>
</ul>

<p>The labeling rates are more commonly known as <strong>queue rates</strong>.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="Posterior"></a></p>
<h3 id="25-the-posterior">2.5 The posterior</h3>
<p>Armed with the prior probability, the likelihood, and the model evidence, we can compute the posterior probability from Bayes‚Äô theorem.  There are four combinations of truth and label:</p>

<ul>
  <li>
    <p>The <strong>positive predictive value</strong>, also known as <strong>precision</strong>: <span>‚Äã<script type="math/tex">\mathit{ppv}\equiv p(\lambda_{1}\mid\ell_{1}) = \frac{p(\ell_{1}\mid\lambda_{1})\, \pi_{1}}{p_{1}} </script></span>,</p>
  </li>
  <li>
    <p>The <strong>negative predictive value</strong>: <span>‚Äã<script type="math/tex">\mathit{npv}\equiv p(\lambda_{0}\mid\ell_{0}) = \frac{p(\ell_{0}\mid\lambda_{0})\, \pi_{0}}{p_{0}} </script></span>,</p>
  </li>
  <li>
    <p>The <strong>false discovery rate</strong>: <span>‚Äã<script type="math/tex">\mathit{fdr}\equiv p(\lambda_{0}\mid\ell_{1}) = 1 - \mathit{ppv}</script></span>, and</p>
  </li>
  <li>
    <p>The <strong>false omission rate</strong>: <span>‚Äã<script type="math/tex">\mathit{for}\equiv p(\lambda_{1}\mid\ell_{0}) = 1 - \mathit{npv} </script></span>.</p>
  </li>
</ul>

<p>The precision, for example, quantifies the predictive value of a positive label by answering the question: If we see a positive label, what is the (posterior) probability that the true class is positive? (Note that the predictive values are not intrinsic properties of the classifier since they depend on the prevalence. Sometimes they are ‚Äústandardized‚Äù by evaluating them at <span>‚Äã<script type="math/tex"> \pi_{0} = \pi_{1} = 1/2 </script></span>.)</p>

<p>Figure 1 shows how the posterior probabilities <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span>, <span>‚Äã<script type="math/tex">\mathit{npv}</script></span>, <span>‚Äã<script type="math/tex">\mathit{fdr}</script></span> and <span>‚Äã<script type="math/tex">\mathit{for}</script></span> are related to the likelihoods <span>‚Äã<script type="math/tex">S_{e}</script></span>, <span>‚Äã<script type="math/tex">S_{p}</script></span>, <span>‚Äã<script type="math/tex">\alpha</script></span> and <span>‚Äã<script type="math/tex">\beta</script></span> via Bayes‚Äô theorem:</p>

<figure class="fullwidth"><img src="/assets/img/blog/008_ConfusionMatrix/FourBayesTheorems.png" /><figcaption>Figure 1: For each combination of class and label, Bayes' theorem connects the corresponding performance metrics of a classifier.</figcaption></figure>

<p>Note that if the operating point of the classifier is chosen in such a way that <span>‚Äã<script type="math/tex">p_{0} = \pi_{0}</script></span>, we‚Äôll have <span>‚Äã<script type="math/tex">\mathit{ppv} = S_{e}</script></span>, <span>‚Äã<script type="math/tex">\mathit{npv} = S_{p}</script></span>, and <span>‚Äã<script type="math/tex">\mathit{fdr}\times \mathit{for} = \alpha\times \beta</script></span>.
<br /></p>
<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="MeasurementPrecision"></a></p>
<h3 id="26-classifier-precision-versus-measurement-precision">2.6 Classifier precision versus measurement precision</h3>
<p>Although we defined precision as a property of a <em>classifier</em> in a given population, there are reasons and ways to extend that definition to precision as a property of a <em>measurement</em>. Consider for example the Covid-19 pandemic. Tests for this disease are effectively classifiers, assigning negative and positive labels to members of the population. The positive predictive value of these tests depends on the prevalence <span>‚Äã<script type="math/tex">\pi_{1}</script></span>, both directly and indirectly through the queue rate <span>‚Äã<script type="math/tex">p_{1}</script></span>. We can make this dependence explicit:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathit{ppv} \;=\; \frac{S_{e}}{S_{e} - \alpha + \alpha\left/\pi_{1}\right.},
</script></div>
<p>showing that <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span> will decrease if <span>‚Äã<script type="math/tex">\pi_{1}</script></span> does. Suppose now that I take the test, and the result is positive. What are the chances that I actually have the disease? One answer would be the positive predictive value. However that number describes the test performance in the population and does not reflect the particular circumstances of my life. If I have no symptoms, and remained at home in isolation for the past two weeks, it seems unlikely that I would actually have the disease. This is where it is useful to remember the Bayesian construction of <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span> as the posterior probability obtained by updating the prior <span>‚Äã<script type="math/tex">\pi_{1}</script></span> with the data from a test result. If we are talking about my particular test result, this prior should fold in more information than just the population prevalence. It should take into account the precautions I took as an individual, which could reduce <span>‚Äã<script type="math/tex">\pi_{1}</script></span> significantly, and therefore also <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span>. The question of how exactly one should estimate prior probabilities is beyond the scope of this blog post, but the distinction between classifier precision and measurement precision is an important one to keep in mind.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="Summary"></a></p>
<h3 id="27-summary">2.7 Summary</h3>
<p>This section introduced twelve quantities:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\pi_{0},\; \pi_{1},\; S_{e},\; S_{p},\; \alpha,\; \beta,\; p_{0},\; p_{1},\; \mathit{ppv},\; \mathit{npv},\; \mathit{fdr},\; \mathit{for}.
</script></div>
<p>Only three of these are independent, say <span>‚Äã<script type="math/tex">\pi_{1}</script></span>, <span>‚Äã<script type="math/tex">\alpha</script></span> and <span>‚Äã<script type="math/tex">\beta</script></span>. To see this, note that by virtue of their definition the first six quantities satisfy three conditions:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\pi_{0} + \pi_{1} = 1,\quad S_{e} + \beta = 1, \quad S_{p} + \alpha = 1,
</script></div>
<p>and the last six can be expressed in terms of the first six: for <span>‚Äã<script type="math/tex">p_{0}</script></span> and <span>‚Äã<script type="math/tex">p_{1}</script></span> we have:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p_{0} = S_{p} \pi_{0} +  \beta \pi_{1}, \quad p_{1} = \alpha \pi_{0} + S_{e} \pi_{1},
</script></div>
<p>and Bayes‚Äô theorem takes care of the remaining four (Figure 1).</p>

<p>The final number of independent quantities matches the number of degrees of freedom of a two-by-two contingency table such as the confusion matrix, which is used to estimate all twelve quantities (see <a href="#MetricsEstimation">below</a>). It also tells us that we only need two numbers to fully characterize a binary classifier (since <span>‚Äã<script type="math/tex">\pi_{1}</script></span> is not a classifier property). As trivial examples, consider the majority and minority classifiers. Assume that class 0 is more prevalent than class 1. Then the majority classifier assigns the label 0 to every instance and has <span>‚Äã<script type="math/tex">\alpha=0</script></span> and <span>‚Äã<script type="math/tex">\beta=1</script></span>. The minority classifier assigns the label 1 to every instance and has <span>‚Äã<script type="math/tex">\alpha=1</script></span> and <span>‚Äã<script type="math/tex">\beta=0</script></span>. These classifiers are of course not very useful. As we will show below, for a classifier to be useful, it must satisfy <span>‚Äã<script type="math/tex"> \alpha + \beta \lt 1 </script></span>.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="JointProbabilities"></a></p>
<h2 id="3-joint-probabilities">3. Joint probabilities</h2>
<p>In some sense the most useful measures of classifier performance are <em>conditional</em> probabilities. By conditioning on something, we do not need to know whether or how that something is realized in order to make a valid statement. For example, by conditioning on true class, we ignore the prevalence when evaluating sensitivity and specificity. Similarly, conditioning on class label allows us to evaluate the predictive values while ignoring the actual queue rate.</p>

<p>In contrast, <em>joint</em> probabilities typically require more specification about the classifier and/or the population of interest, and are therefore less general than conditional probabilities. Nevertheless, by combining joint probabilities one can still obtain useful metrics. Start for example with the joint probability for class label and true class to be positive:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(\ell_{1} \;\&\; \lambda_{1}).
</script></div>
<p>Adding the joint probability for class label to be positive and true class to be negative yields the positive queue rate introduced earlier:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(\ell_{1} \;\&\; \lambda_{1}) \;+\; p(\ell_{1} \;\&\; \lambda_{0})
  \;=\; p(\ell_{1}\mid\lambda_{1})\,\pi_{1} \;+\; p(\ell_{1}\mid\lambda_{0})\,\pi_{0}
  \;=\; p_{1}.
</script></div>
<p>If, instead, we add the joint probability for class label and true class to be negative, we obtain the so-called accuracy:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(\ell_{1} \;\&\; \lambda_{1}) \;+\; p(\ell_{0} \;\&\; \lambda_{0})
  \;=\; p[(\ell_{1} \;\&\; \lambda_{1}) \;\lor\; (\ell_{0} \;\&\; \lambda_{0})]
  \;=\; p(\ell \!=\! \lambda)
  \;=\; {\rm A}.
</script></div>
<p>In terms of quantities introduced previously this is:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
{\rm A}  \;=\; p(\ell_{1} \;\&\; \lambda_{1}) \;+\; p(\ell_{0} \;\&\; \lambda_{0})
        &\;=\; p(\ell_{1}\mid\lambda_{1})\,\pi_{1} \;+\; p(\ell_{0}\mid\lambda_{0})\,\pi_{0}
         \;=\; S_{e}\,\pi_{1} \;+\; S_{p}\,\pi_{0}\\[1mm]
        &\;=\; p(\lambda_{1}\mid\ell_{1})\,p_{1} \;+\; p(\lambda_{0}\mid\ell_{0})\,p_{0}
         \;=\; \mathit{ppv}\,p_{1} \;+\; \mathit{npv}\,p_{0}.
\end{align}
</script></div>
<p>Note the dependence on prevalence or queue rate. An equivalent measure is the <strong>misclassification rate</strong>, defined as one minus the accuracy. A benchmark that is sometimes used is the <strong>null error rate</strong>, defined as the misclassification rate of a classifier that always predicts the majority class. It is equal to <span>‚Äã<script type="math/tex">\min\{\pi_{0}, \pi_{1}\}</script></span>. The complement of the null error rate is the <strong>null accuracy</strong>, which is equal to <span>‚Äã<script type="math/tex">\max\{\pi_{0}, \pi_{1}\}</script></span>.</p>

<p>Note that null accuracy and null error rate are not necessarily good, or even reasonable benchmarks. In a highly imbalanced dataset for example, the classifier that always predicts the majority class will appear to have excellent performance even though it does not use any <em>relevant</em> information in the data.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="LikelihoodRatios"></a></p>
<h2 id="4-likelihood-ratios">4. Likelihood ratios</h2>
<p>In some fields it is customary to work with ratios of probabilities rather than with the probabilities themselves.  Thus one defines:</p>

<ul>
  <li>
    <p>The <strong>positive likelihood ratio</strong>: <span>‚Äã<script type="math/tex">{\rm lr+} \equiv \frac{p(\ell_{1}\mid\lambda_{1})}{p(\ell_{1}\mid\lambda_{0})} = \frac{S_{e}}{1-S_{p}}</script></span>, which represents the odds of the true class being positive if the label is positive (in medical statistics for example, this would be the odds of disease if the test result is positive).</p>
  </li>
  <li>
    <p>The <strong>negative likelihood ratio</strong>: <span>‚Äã<script type="math/tex">{\rm lr-} \equiv \frac{p(\ell_{0}\mid\lambda_{1})}{p(\ell_{0}\mid\lambda_{0})} = \frac{1-S_{e}}{S_{p}}</script></span>, which represents the odds of the true class being positive if the label is negative (in medical statistics, the odds of disease if the test result is negative).</p>
  </li>
</ul>

<p>Finally, it is instructive to take the ratio of these two likelihood ratios. This yields</p>

<ul>
  <li>The <strong>diagnostic odds ratio</strong>: <span>‚Äã<script type="math/tex">\mathit{dor} \equiv \frac{\rm lr+}{\rm lr-} = \frac{S_{e}}{1-S_{p}}\frac{S_{p}}{1-S_{e}}</script></span>.</li>
</ul>

<p>If <span>‚Äã<script type="math/tex">\mathit{dor} \gt 1</script></span>, the classifier is deemed useful, since this means that, in our medical example, the odds of disease are higher when the test result is positive than when it is negative. If <span>‚Äã<script type="math/tex">\mathit{dor}=1</script></span> the classifier is useless, but if <span>‚Äã<script type="math/tex">\mathit{dor}\lt 1</script></span> the classifier is worse than useless, it is misleading. In this case one would be better off swapping the labels on the classifier output.</p>

<p>Note that the ratios <span>‚Äã<script type="math/tex">{\rm lr+}</script></span>, <span>‚Äã<script type="math/tex">{\rm lr-}</script></span>, and <span>‚Äã<script type="math/tex">\mathit{dor}</script></span> are all independent of prevalence.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="ClassifierUsefulness"></a></p>
<h2 id="5-when-is-a-classifier-useful">5. When is a classifier useful?</h2>
<p>The usefulness condition <span>‚Äã<script type="math/tex">\fbox{$\mathit{dor} \gt 1$}</script></span> is mathematically equivalent to any of the following conditions:</p>

<ol>
  <li>
    <p><span>‚Äã<script type="math/tex">\fbox{$S_{e}\gt 1 - S_{p}$}</script></span> The sensitivity must be larger than one minus the specificity. Equivalently, the probability of detecting a positive-class instance must be larger than the probability of mislabeling a negative-class instance.  Reformulating in terms of Type-I and II error rates this condition becomes <span>‚Äã<script type="math/tex">\fbox{$\beta \lt 1 - \alpha$}</script></span>.</p>
  </li>
  <li>
    <p><span>‚Äã<script type="math/tex">\fbox{$S_{e} \gt p_{1}$}</script></span> The probability of encountering a positively labeled instance must be larger within the subset of positive-class instances than within the entire population. Similarly: <span>‚Äã<script type="math/tex">\fbox{$S_{p} \gt p_{0}$}</script></span>, <span>‚Äã<script type="math/tex">\fbox{$\alpha \lt p_{1}$}</script></span>, and <span>‚Äã<script type="math/tex">\fbox{$\beta \lt p_{0}$}</script></span>.</p>
  </li>
  <li>
    <p><span>‚Äã<script type="math/tex">\fbox{$\mathit{ppv} \gt \pi_{1}$}</script></span> The precision must be larger than the prevalence. In other words, the probability for an instance to belong to the positive class must be larger within the subset of instances with a positive label than within the entire population. If this is not the case, the classifier adds no useful information. Similarly: <span>‚Äã<script type="math/tex">\fbox{$\mathit{npv} \gt \pi_{0}$}</script></span>, <span>‚Äã<script type="math/tex">\fbox{$\mathit{fdr} \lt \pi_{0}$}</script></span>, and <span>‚Äã<script type="math/tex">\fbox{$\mathit{for} \lt \pi_{1}$}</script></span>.</p>
  </li>
</ol>

<p>The classifier usefulness condition also puts a bound on the accuracy (but is <em>not</em> equivalent to it):</p>
<div class="mathblock"><script type="math/tex; mode=display">
{\rm A} \;=\; S_{e}\,\pi_{1} \;+\; S_{p}\,\pi_{0}
        \;>\; p_{1}\,\pi_{1} \;+\; p_{0}\,\pi_{0}.
</script></div>

<p>Since <span>‚Äã<script type="math/tex">p_{0} + p_{1} = 1</script></span>, the right-hand side of the above inequality is bounded between <span>‚Äã<script type="math/tex">\min(\pi_{0}, \pi_{1})</script></span> and <span>‚Äã<script type="math/tex">\max(\pi_{0}, \pi_{1})</script></span>. The upper bound is actually the accuracy of the majority classifier, which assigns the majority label to every instance. Note that the majority classifier itself is not a useful classifier, since its accuracy is <em>equal</em> to its corresponding bound, not <em>strictly higher</em> than it. In addition, it is entirely possible for a useful classifier to have an accuracy below that of the useless majority classifier. To illustrate this last point, consider the minority classifier, which assigns the minority label to every instance. This is a useless classifier, with accuracy <span>‚Äã<script type="math/tex">\pi_{1}</script></span> if we assume that <span>‚Äã<script type="math/tex">\pi_{1} \lt \pi_{0}</script></span>. To improve it, let‚Äôs flip the label of one arbitrarily chosen majority instance. The accuracy of this new classifier is <span>‚Äã<script type="math/tex">\pi_{1} + 1/N</script></span>, with <span>‚Äã<script type="math/tex">N</script></span> the size of the total population (assumed finite in this example). This is larger than the accuracy bound, <span>‚Äã<script type="math/tex">p_{1}\pi_{1} + p_{0}\pi_{0} = \pi_{1} + 1/N - 2\pi_{1}/N</script></span>. Hence the new classifier is useful, even though for <span>‚Äã<script type="math/tex">N</script></span> large enough its accuracy will be lower than that of the majority classifier. Of course for other metrics the new classifier performs better than the majority one, for example sensitivity and the predictive values.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="ScoreBasedMetrics"></a></p>
<h2 id="6-classifier-scores">6. Classifier scores</h2>
<p>As mentioned earlier, classifiers usually produce a score <span>‚Äã<script type="math/tex">q</script></span> to quantify the likelihood that an instance belongs to the positive class. The performance metrics introduced so far can be defined in terms of this score, but they only need it in a binary way, through the class label <span>‚Äã<script type="math/tex">\ell</script></span>: <span>‚Äã<script type="math/tex">\ell=1</script></span> if <span>‚Äã<script type="math/tex">q\ge q_{T}</script></span>, otherwise <span>‚Äã<script type="math/tex">\ell=0</script></span>, for some predefined threshold <span>‚Äã<script type="math/tex">q_{T}</script></span>. The next two performance metrics we discuss use score values without reference to <span>‚Äã<script type="math/tex">q_{T}</script></span>.</p>

<p><a name="ROC"></a></p>
<h3 id="61-the-receiver-operating-characteristic-roc">6.1 The Receiver Operating Characteristic (ROC)</h3>
<p>The <strong>Receiver Operating Characteristic</strong>, or <strong>ROC</strong>, is defined as the curve of true positive rate versus false positive rate (or sensitivity versus one-minus-specificity). An example is shown in the figure below:</p>
<figure><figcaption>Figure 2. Left: arbitrary classifier score distributions used for illustration. Right: corresponding ROC curve, obtained by varying the classifier threshold over the range of scores. In this example the area under the ROC is about 0.815.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/scores_distribution_and_roc.png" /></figure>
<p>Note how the main diagonal line <span>‚Äã<script type="math/tex">S_{e} = 1 - S_{p}</script></span> serves as baseline in the ROC plot, in accordance with the classifier usefulness condition <span>‚Äã<script type="math/tex">S_{e} \gt 1 - S_{p}</script></span> discussed in <a href="#ClassifierUsefulness">section 5</a>.</p>

<p>The area under the ROC curve, or <strong>AUROC</strong>, is a performance metric that‚Äôs independent of prevalence and does not require a choice of threshold <span>‚Äã<script type="math/tex">q_{T}</script></span> on the classifier score <span>‚Äã<script type="math/tex">q</script></span>. In other words, it does not depend on the chosen operating point of the classifier. It can be shown that the AUROC is equal to the probability for a random positive instance to be scored higher than a random negative instance.</p>

<p>An immediate interpretation of the AUROC is that it quantifies how well the score distributions for positive and negative instances are separated. If the score distribution for positives is to the right of that for negatives, and there is no overlap, the AUROC equals 1. If the positives are all to the left of the negatives, the AUROC equals 0, and if the two distributions overlap each other exactly, the AUROC equals 0.5.</p>

<p>The AUROC is related to expectation values of sensitivity, specificity, and accuracy (see <a href="#ExpectationValues">appendix 11.1</a> for a proof):</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\mathbb{E}\left[S_{e}\right] \;&=\; \pi_{0}\, \textrm{AUROC} \;+\; \frac{\pi_{1}}{2},\\
\mathbb{E}\left[S_{p}\right] \;&=\; \pi_{1}\, \textrm{AUROC} \;+\; \frac{\pi_{0}}{2},\\
\mathbb{E}\left[A\right]     \;&=\; 2\,\pi_{0}\,\pi_{1}\,\textrm{AUROC} \,+\, \frac{\pi_{0}^{2} + \pi_{1}^{2}}{2},
\end{align}
</script></div>
<p>where the expectation values are over an ensemble of identical classifiers with operating points that are randomly drawn from the distribution of scores of the population of interest. These equations demonstrate how the AUROC aggregates classifier performance information in a way that‚Äôs independent of prevalence and operating point (see also <a href="https://www.quora.com/Machine-Learning-What-is-an-intuitive-explanation-of-AUC/answer/Peter-Flach">this answer</a> by Peter Flach on Quora).</p>

<p>As we saw earlier, only three quantities are needed to determined all the performance characteristics of a classifier. We can take these to be the prevalence <span>‚Äã<script type="math/tex">\pi_{1}</script></span>, the sensitivity <span>‚Äã<script type="math/tex">S_{e}</script></span>, and the specificity <span>‚Äã<script type="math/tex">S_{p}</script></span>. Thus, for a given value of the prevalence, each point on the ROC is associated with a unique value of, say, the precision. We can illustrate this by superimposing a colored map of precision on top of the ROC:</p>
<figure><figcaption>Figure 3. Left: ROC curve for a classifier in a balanced dataset, with a color map of the precision superimposed. Right: ROC curve and precision for the same classifier in the same dataset but with a lower prevalence.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/precision_color_maps.png" /></figure>
<p>The figure shows how the ROC curve is independent of prevalence, whereas the precision is not.</p>

<p><a name="PRC"></a></p>
<h3 id="62-the-precision-recall-curve-prc">6.2 The Precision-Recall Curve (PRC)</h3>
<p>In cases where there is severe class imbalance in the data, the ROC may give too optimistic a view of classifier performance, because the true and false positive rates are both independent of prevalence, whereas precision deteriorates at low prevalence. It may therefore be preferable to plot a precision versus recall curve (or PRC). In the two illustrations below, the class distributions are the same, and only the class-1 prevalence is different. It can be seen that the ROC is independent of prevalence and looks quite good. On the other hand, the precision versus recall curve shows that the classifier performance leaves quite a bit to be desired in the case of imbalanced data. One can also calculate the area under the precision versus recall curve (or AUPRC) to summarize this effect.</p>
<figure class="fullwidth"><img src="/assets/img/blog/008_ConfusionMatrix/scores_distribution_roc_prc_0.png" /><figcaption>Figure 4. Balanced dataset:  classifier score distributions (a), metrics vs. threshold (b), ROC (c), and PRC (d). The areas under the ROC and the PRC are both about 0.92.</figcaption></figure>
<figure class="fullwidth"><img src="/assets/img/blog/008_ConfusionMatrix/scores_distribution_roc_prc_1.png" /><figcaption>Figure 5. Imbalanced dataset: classifier score distributions (a), metrics vs. threshold (b), ROC (c), and PRC(d). In this case the area under the ROC is still about 0.92 whereas that under the PRC is now 0.65.</figcaption></figure>

<p><a name="ScoresAndUsefulnessCondition"></a></p>
<h3 id="63-classifier-scores-and-the-usefulness-conditions">6.3 Classifier Scores and the Usefulness Conditions</h3>
<p>The classifier usefulness conditions discussed in <a href="#ClassifierUsefulness">section 5</a> apply to every threshold setting of a classifier with scores. This can lead to non-trivial situations, for example when a few negative-class instances are scored higher than the highest-score positive-class instance. To illustrate this, we consider an exaggerated example in the figures below.</p>
<figure class="fullwidth"><img src="/assets/img/blog/008_ConfusionMatrix/scores_distribution_roc_prc_2.png" /><figcaption>Figure 6. Classifier score distributions in an imbalanced dataset where some class-0 instances extend beyond the class-1 distribution (a), metrics vs. threshold (b), ROC (c), and PRC (d). The area under the ROC is about 0.78, and the area under the PRC is 0.43.</figcaption></figure>
<p>Note how the classifier usefulness conditions are violated. In panel (b), the precision dips below the prevalence, and the recall below the queue rate, at a threshold of about 0.61, where the recall equals 0.123. In panel (c) true positive rates lower than 0.123 are below the corresponding false positive rates, and in panel (d) the precision is lower than the prevalence when the recall is lower than 0.123. In the last three panels, the classifier is useless when its threshold is in a region where one of the colored curves has crossed into a hatched area of the same color. The crossing points are marked by red dots.</p>

<p>It is important to understand that the curves discussed in this section don‚Äôt show functional relationships, but rather operating characteristics. Consider the plot of precision versus recall for example. The definition of precision implies that, functionally, it is an increasing function of recall. This is easy to see:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathit{ppv} \;=\; \frac{S_{e}\, \pi_{1}}{p_{1}}
             \;=\; \frac{S_{e}\, \pi_{1}}{S_{e}\,\pi_{1} \,+\, \alpha\,\pi_{0}}
             \;=\; \frac{\pi_{1}}{\pi_{1} \,+\, \alpha\,\pi_{0}\left/S_{e}\right.}
</script></div>
<p>And yet, the plot shows a decreasing function! This is because each point on the plotted curves represents an operating point, i.e., a choice of classifier threshold, and this choice affects not only <span>‚Äã<script type="math/tex">S_{e}</script></span>, but also <span>‚Äã<script type="math/tex">\alpha</script></span>. If we just wanted to show how <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span> depends on <span>‚Äã<script type="math/tex">S_{e}</script></span>, we would keep <span>‚Äã<script type="math/tex">\alpha</script></span> constant. In fact, what the plot shows is the joint variation of precision and recall as the threshold is changed.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="OperatingPoints"></a></p>
<h2 id="7-classifier-operating-points">7. Classifier operating points</h2>
<p>The area under the ROC or under the PRC is a good performance measure when one wishes to decide between different types of classification algorithms (e.g., random forest versus na√Øve Bayes versus support vector machine), because this area is independent of the actual classifier threshold. Once an algorithm has been selected however, one needs to choose an operating point, that is, a threshold at which to operate the classifier. This is not trivial. If, for example, one is mostly interested in precision, choosing the threshold that maximizes precision will typically result in zero queue rate. Similarly, maximizing recall typically leads to 100% queue rate (see the examples in the section on <a href="#TrendsAndBaselines">trends</a> below). The best approach is to find the threshold that maximizes a balanced combination of metrics, such as precision and recall, or true and false positive rates. The following two options are common:</p>

<ol>
  <li>
    <p>The <span>‚Äã<script type="math/tex">F_{\beta}</script></span> score, which is a weighted harmonic mean of precision and recall: <span>‚Äã<script type="math/tex">F_{\beta} \equiv \frac{(1+\beta^{2})\,\times\,\mathit{ppv}\,\times\, S_{e}}{\beta^{2}\,\times\,\mathit{ppv}\, +\, S_{e}} </script></span>. Usually <span>‚Äã<script type="math/tex">\beta</script></span> is set to 1, which yields the regular harmonic mean of precision and recall.</p>
  </li>
  <li>
    <p>Youden‚Äôs <span>‚Äã<script type="math/tex">J</script></span> statistic, which equals the true positive rate minus the false positive rate: <span>‚Äã<script type="math/tex">J = S_{e} - \alpha</script></span>.</p>
  </li>
</ol>

<p>In order to avoid bias, the maximization of <span>‚Äã<script type="math/tex">F_{\beta}</script></span> or <span>‚Äã<script type="math/tex">J</script></span> as a function of threshold should be done on a test dataset that‚Äôs different from the one used to evaluate the resulting classifier‚Äôs performance.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="MetricsEstimation"></a></p>
<h2 id="8-performance-metric-estimation">8. Performance metric estimation</h2>
<p>The performance metrics can be estimated from a two-by-two contingency table known as the confusion matrix. It is obtained by applying the classifier to a test data set different from the training data set. Here is standard notation for this matrix:</p>

<figure class="fullwidth"><img src="/assets/img/blog/008_ConfusionMatrix/ConfusionMatrix.png" /><figcaption>Figure 7: Estimated confusion matrix. The notation tp stands for "number of true positives", fn for "number of false negatives", and so on.</figcaption></figure>

<p>The rows correspond to class labels, the columns to true classes. We then have the following approximations:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{aligned}
\mbox{Prevalence :} & \quad \pi_{1} &&\equiv\pi(\lambda_{1}) &&\approx \dfrac{\rm fn+tp}{\rm fn+tp+fp+tn}\\[1mm]

\mbox{Queue rate :} & \quad p_{1} &&\equiv p(\ell_{1}) &&\approx \dfrac{\rm fp+tp}{\rm fn+tp+fp+tn}\\[4mm]

\mbox{True-positive rate, sensitivity, recall :} & \quad S_{e} &&\equiv p(\ell_{1}\mid\lambda_{1}) &&\approx \dfrac{\rm tp}{\rm tp+fn}\\[1mm]

\mbox{True-negative rate, specificity :} & \quad S_{p} &&\equiv p(\ell_{0}\mid\lambda_{0}) &&\approx \dfrac{\rm tn}{\rm tn + fp}\\[1mm]

\mbox{False-positive rate, Type-I error rate :} & \quad \alpha &&\equiv p(\ell_{1}\mid\lambda_{0})&&\approx \dfrac{\rm fp}{\rm tn + fp}\\[1mm]

\mbox{False-negative rate, Type-II error rate :} & \quad \beta &&\equiv p(\ell_{0}\mid\lambda_{1})&&\approx \dfrac{\rm fn}{\rm tp + fn}\\[4mm]

\mbox{Positive predictive value, precision :} & \quad \mathit{ppv} &&\equiv p(\lambda_{1}\mid\ell_{1}) &&\approx \dfrac{\rm tp}{\rm tp + fp}\\[1mm]

\mbox{Negative predictive value :} & \quad \mathit{npv} &&\equiv p(\lambda_{0}\mid\ell_{0}) &&\approx \dfrac{\rm tn}{\rm tn + fn}\\[1mm]

\mbox{False-discovery rate :} & \quad \mathit{fdr} &&\equiv p(\lambda_{0}\mid\ell_{1}) &&\approx \dfrac{\rm fp}{\rm tp + fp}\\[1mm]

\mbox{False-omission rate :} & \quad \mathit{for} &&\equiv p(\lambda_{1}\mid\ell_{0}) &&\approx \dfrac{\rm fn}{\rm tn + fn}\\[4mm]

\mbox{Positive likelihood ratio :} & \quad {\rm lr+} &&\equiv \frac{p(\ell_{1}\mid\lambda_{1})}{p(\ell_{1}\mid\lambda_{0})}&&\approx \frac{\rm tp}{\rm tp+fn}\; \frac{\rm tn+fp}{\rm fp}\\[1mm]

\mbox{Negative likelihood ratio :} & \quad {\rm lr-} &&\equiv \frac{p(\ell_{0}\mid\lambda_{1})}{p(\ell_{0}\mid\lambda_{0})}&&\approx \frac{\rm fn}{\rm tp+fn}\; \frac{\rm tn+fp}{\rm tn}\\[1mm]

\mbox{Diagnostic odds ratio :} & \quad \mathit{dor} &&\equiv \frac{\rm lr+}{\rm lr-} &&\approx \frac{\rm tp}{\rm fp} \frac{\rm tn}{\rm fn}\\[4mm]

\mbox{Accuracy :} & \quad A &&\equiv p(\ell=\lambda) &&\approx \dfrac{\rm tp + tn}{\rm tp + tn + fp + fn}
\end{aligned}
</script></div>
<p>There are no simple expressions for the AUROC and AUPRC, since these require integration under the ROC and PRC, respectively.</p>

<p>Note that the relations between probabilities derived in the previous sections also hold between their estimates.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="TrendsAndBaselines"></a></p>
<h2 id="9-trends-and-baselines">9. Trends and baselines</h2>
<p>For classifiers that generate scores it is useful to understand how various metrics change as the classifier threshold on the score is increased. This will of course depend on the actual score distributions of true class-0 and true class-1 instances. In the following four illustrations we keep these two distributions identical: one a Gaussian with mean 0.25 and width 0.25, and the other a Gaussian with mean 0.75 and width 0.25. What changes is the sample composition: balanced or imbalanced, high- or low-statistics; and which class is represented by which Gaussian.</p>

<p>Case 1: Balanced, high-statistics dataset, with class-1 scores mostly above class-0 scores</p>

<figure><figcaption>Figure 7. The recall and queue rate both decrease, whereas the precision increases. As noted previously, if the classifier threshold is chosen such that queue rate equals prevalence, precision will equal recall.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/trends_5000_5000_01.png" /></figure>

<p>Case 2: Balanced, high-statistics dataset, with class-1 scores mostly <em>below</em> class-0 scores</p>

<figure><figcaption>Figure 8. Compared to Case 1 the recall is still decreasing with threshold, but the precision is now decreasing instead of increasing. The baselines are now <em>above</em> their corresponding metrics, indicating that the classifier is misleading.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/trends_5000_5000_10.png" /></figure>

<p>Case 3: Balanced, low-statistics dataset, with class-1 scores mostly above class-0 scores</p>

<figure><figcaption>Figure 9. The curves are essentially the same as for Case 1, but with pronounced statistical fluctuations. Note that the recall never fluctuates up: it stays even or decreases. In contrast, the precision fluctuates both up and down.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/trends_0020_0020_01.png" /></figure>

<p>Case 4: Imbalanced, high-statistics dataset, with class-1 scores mostly above class-0 scores</p>

<figure><figcaption>Figure 10. Compared to Case 1, the recall hasn't changed, but its baseline, the queue rate, has shifted down as a whole. The precision has also shifted down.</figcaption><img src="/assets/img/blog/008_ConfusionMatrix/trends_9500_0500_01.png" /></figure>

<p>See appendix <a href="#MetricsVsThreshold">11.2</a> for a more detailed explanation of the above trends.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="EffectOfPrevalence"></a></p>
<h2 id="10-effect-of-prevalence-on-classifier-training-and-testing">10. Effect of prevalence on classifier training and testing</h2>
<p>In this post I have tried to be careful in pointing out which performance metrics depend on the prevalence, and which don‚Äôt. The reason is that prevalence is a population or sample property, whereas here we are interested in classifier properties. Of course, real-world classifiers must be trained and tested on finite samples, and this has implications for the actual effect of prevalence on classifier performance:</p>

<ul>
  <li>
    <p>When <strong>training</strong> a classifier, changing the prevalence in the training data <em>may affect</em> properties such as the sensitivity and specificity, depending on the characteristics of the training algorithm.</p>
  </li>
  <li>
    <p>When <strong>testing</strong> a trained classifier, changing the prevalence in the test data <em>will not affect</em> properties such as the sensitivity and specificity (within statistical fluctuations).</p>
  </li>
</ul>

<p>To illustrate this point I went back to a <a href="/articles/16/RandomForestsWiMLDS">previous post</a> on predicting flight delays with a random forest. The prevalence of the given flight delay data set is about 20%, but can be increased by randomly dropping class 0 instances (non-delayed flights). The table below shows the effect of changing the prevalence in this way in the <em>training</em> data:
<br /><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Training set prevalence</th>
      <th style="text-align: center">Sensitivity</th>
      <th style="text-align: center">Specificity</th>
      <th style="text-align: center">Accuracy</th>
      <th style="text-align: center">AUROC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center">0.16</td>
      <td style="text-align: center">0.96</td>
      <td style="text-align: center">0.72</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.42</td>
      <td style="text-align: center">0.29</td>
      <td style="text-align: center">0.91</td>
      <td style="text-align: center">0.72</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.44</td>
      <td style="text-align: center">0.46</td>
      <td style="text-align: center">0.82</td>
      <td style="text-align: center">0.71</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.46</td>
      <td style="text-align: center">0.58</td>
      <td style="text-align: center">0.72</td>
      <td style="text-align: center">0.68</td>
      <td style="text-align: center">0.71</td>
    </tr>
    <tr>
      <td style="text-align: center">0.48</td>
      <td style="text-align: center">0.63</td>
      <td style="text-align: center">0.67</td>
      <td style="text-align: center">0.66</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.73</td>
      <td style="text-align: center">0.57</td>
      <td style="text-align: center">0.62</td>
      <td style="text-align: center">0.71</td>
    </tr>
  </tbody>
</table>

<p>To be clear, for each row it is the same classifier (same hyper-parameters: number of trees, maximum tree depth, etc.) that is being fit to a training data set with varying prevalence. The sensitivity, specificity, accuracy, and AUROC are then measured on a testing data set with a constant prevalence of 30%. Note how the sensitivity and specificity, <em>as measured on an independent test data set</em>, vary with training set prevalence. The AUROC, on the other hand, remains stable.</p>

<p>Next, let‚Äôs look at the effect of changing the prevalence in the <em>testing</em> data:
<br /><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Testing set prevalence</th>
      <th style="text-align: center">Sensitivity</th>
      <th style="text-align: center">Specificity</th>
      <th style="text-align: center">Accuracy</th>
      <th style="text-align: center">AUROC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0.20</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.78</td>
      <td style="text-align: center">0.73</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.30</td>
      <td style="text-align: center">0.48</td>
      <td style="text-align: center">0.80</td>
      <td style="text-align: center">0.70</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.40</td>
      <td style="text-align: center">0.45</td>
      <td style="text-align: center">0.82</td>
      <td style="text-align: center">0.67</td>
      <td style="text-align: center">0.72</td>
    </tr>
    <tr>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.46</td>
      <td style="text-align: center">0.80</td>
      <td style="text-align: center">0.63</td>
      <td style="text-align: center">0.71</td>
    </tr>
    <tr>
      <td style="text-align: center">0.60</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.79</td>
      <td style="text-align: center">0.61</td>
      <td style="text-align: center">0.71</td>
    </tr>
  </tbody>
</table>

<p>Here the prevalence in the training data was set at 0.45. The sensitivity and specificity appear much more stable; residual variations may be due to the effect on the training data set of the procedure used to vary the testing set prevalence.</p>

<p>Training data sets are often unbalanced, with prevalences even lower than in the example discussed here. Modules such as scikit-learn‚Äôs <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><code class="language-plaintext highlighter-rouge">RandomForestClassifier</code></a> provide a <code class="language-plaintext highlighter-rouge">class_weight</code> option, which can be set to <code class="language-plaintext highlighter-rouge">balanced</code> or <code class="language-plaintext highlighter-rouge">balanced_subsample</code> in order to assign a larger weight to the minority class. This re-weighting is applied both to the GINI criterion used for finding splits at decision tree nodes and to class votes in the terminal nodes. The sensitivity and specificity of a classifier, as well as its accuracy, will depend on the setting of the <code class="language-plaintext highlighter-rouge">class_weight</code> option. For scoring purposes when optimizing a classifier, AUROC tends to be a much more robust property.</p>
<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>

<p><a name="Appendix"></a></p>
<h2 id="11-mathematical-appendix">11. Mathematical Appendix</h2>
<p>This appendix contains mathematical proofs of some of the statements made in the body of this post.</p>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="ExpectationValues"></a></p>
<h3 id="111-expectation-values-of-sensitivity-specificity-and-accuracy">11.1 Expectation values of sensitivity, specificity, and accuracy</h3>
<p>To see how the AUROC is related to an expectation value of the sensitivity, note that for a given threshold <span>‚Äã<script type="math/tex"> q_{T} </script></span> on the classifier output score, we can write the sensitivity as:</p>
<div class="mathblock"><script type="math/tex; mode=display">
S_{e}(q_{T}) \;=\; \mathbb{P}\left[Q_{1}\ge q_{T}\right],
</script></div>
<p>where <span>‚Äã<script type="math/tex">Q_{1}</script></span> is a <em>random variable distributed as the classifier scores of positive instances</em>. Let us now compute the expectation value of <span>‚Äã<script type="math/tex"> S_{e}(q_{T}) </script></span> where <span>‚Äã<script type="math/tex"> q_{T} </script></span> is varied over the distribution of classifier scores of all instances in the population of interest, class 0 and class 1. This score distribution can be modeled by a random variable <span>‚Äã<script type="math/tex"> Q^{\prime} </script></span> defined as follows:</p>
<div class="mathblock"><script type="math/tex; mode=display">
Q^{\prime} \;\equiv\; \mathbb{1}(U \le \pi_{0})\, Q^{\prime}_{0} \;+\;
                      \mathbb{1}(U \gt \pi_{0})\, Q^{\prime}_{1},
</script></div>
<p>where <span>‚Äã<script type="math/tex">\mathbb{1}(A)</script></span> is the indicator function of set <span>‚Äã<script type="math/tex">A</script></span>, and <span>‚Äã<script type="math/tex"> U </script></span>, <span>‚Äã<script type="math/tex"> Q^{\prime}_{0} </script></span> and <span>‚Äã<script type="math/tex"> Q^{\prime}_{1} </script></span> are independent random variables, with <span>‚Äã<script type="math/tex"> U </script></span> being uniform between 0 and 1, and <span>‚Äã<script type="math/tex"> Q^{\prime}_{0} </script></span>, <span>‚Äã<script type="math/tex"> Q^{\prime}_{1} </script></span> following the score distributions of class-0, resp. class-1 instances. Extending the definition of <span>‚Äã<script type="math/tex">S_{e}</script></span> to a function of a random threshold:</p>
<div class="mathblock"><script type="math/tex; mode=display">
S_{e}(Q^{\prime}) \;\equiv\; \mathbb{P}\left[Q_{1} \ge Q^{\prime}\mid Q^{\prime}\right]
                  \; = \; \mathbb{E}\left[\mathbb{1}(Q_{1} \ge Q^{\prime}) \mid Q^{\prime}\right],
</script></div>
<p>we then have:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align}
\mathbb{E}\left[S_{e}\right]
\;&\equiv\; \mathbb{E}\left[ \;\mathbb{E}\left[ \mathbb{1}(Q_{1} \ge Q^{\prime})\mid Q^{\prime} \right]\; \right]\\[1mm]
\;&=\; \mathbb{E}\left[ \mathbb{1}(Q_{1} \ge Q^{\prime})\right]\quad\textrm{by the law of total expectation,}\\[1mm]
\;&=\; \mathbb{E}\left[ \mathbb{1}(U \le \pi_{0})\;\mathbb{1}(Q_{1} \ge Q^{\prime}_{0}) \;+\; \mathbb{1}(U > \pi_{0})\;\mathbb{1}(Q_{1} \ge Q^{\prime}_{1}) \right]\quad\textrm{by definition of $Q^{\prime}$,}\\[1mm]
\;&=\; \mathbb{E}\left[ \mathbb{1}(U \le \pi_{0})\;\mathbb{1}(Q_{1} \ge Q^{\prime}_{0})\right] \;+\; \mathbb{E}\left[ \mathbb{1}(U > \pi_{0})\;\mathbb{1}(Q_{1} \ge Q^{\prime}_{1}) \right]\quad\textrm{by linearity of the expectation operator,}\\[1mm]
\;&=\; \mathbb{E}\left[ \mathbb{1}(U \le \pi_{0})\right] \; \mathbb{E}\left[ \mathbb{1}(Q_{1} \ge Q^{\prime}_{0})\right] \;+\; \mathbb{E}\left[ \mathbb{1}(U > \pi_{0})\right] \; \mathbb{E}\left[ \mathbb{1}(Q_{1} \ge Q^{\prime}_{1}) \right]\quad\textrm{by independence of $U$, $Q_{1}$, $Q_{0}^{\prime}$, $Q_{1}^{\prime}$,}\\[1mm]
\;&=\; \pi_{0}\,\mathbb{P}\left(Q_{1} \ge Q^{\prime}_{0}\right) \;+\; \pi_{1}\,\mathbb{P}\left(Q_{1} \ge Q^{\prime}_{1}\right)\quad\textrm{by definition of $\mathbb{P}$ in terms of $\mathbb{E}$,}\\[1mm]
\;&=\; \pi_{0}\, \textrm{AUROC} \;+\; \frac{\pi_{1}}{2}\quad\textrm{by definition of AUROC.}
\end{align}
</script></div>
<p>One can similarly show that:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathbb{E}\left[S_{p}\right] \;=\; \pi_{1}\, \textrm{AUROC} \;+\; \frac{\pi_{0}}{2}.
</script></div>
<p>Using the expression for the accuracy A in terms of <span>‚Äã<script type="math/tex">S_{e}</script></span> and <span>‚Äã<script type="math/tex">S_{p}</script></span>,
and the linearity of the expectation operator, we then find:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathbb{E}\left[A\right] \;=\; 2\,\pi_{0}\,\pi_{1}\,\textrm{AUROC} \,+\, \frac{\pi_{0}^{2} + \pi_{1}^{2}}{2}.
</script></div>

<div style="text-align: right"><a href="#TopOfPage">Back to Top</a></div>
<p><a name="MetricsVsThreshold"></a></p>
<h3 id="112-performance-metrics-versus-threshold">11.2 Performance metrics versus threshold</h3>
<p>To understand the behavior of the performance metrics in section <a href="#TrendsAndBaselines">9</a>, consider what happens when we increase the classifier threshold <span>‚Äã<script type="math/tex">t</script></span> by an amount <span>‚Äã<script type="math/tex">\Delta t \gt 0</script></span>. A number <span>‚Äã<script type="math/tex">\Delta p + \Delta n</script></span> of instances will see their labels flip from 1 to 0, where <span>‚Äã<script type="math/tex">\Delta p</script></span> are class-1 and <span>‚Äã<script type="math/tex">\Delta n</script></span> are class-0. The effect on the confusion matrix elements is:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
{\rm tp} &\quad\longrightarrow\quad {\rm tp} - \Delta p\\
{\rm fn} &\quad\longrightarrow\quad {\rm fn} + \Delta p\\
{\rm tn} &\quad\longrightarrow\quad {\rm tn} + \Delta n\\
{\rm fp} &\quad\longrightarrow\quad {\rm fp} - \Delta n
\end{align*}
</script></div>
<p>Applying these transformations to the metrics in section <a href="#MetricsEstimation">8</a> yields the following results.</p>

<p>For the queue rates:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
p_{1} &= \frac{\rm tp+fp}{\rm tn+fn+tp+fp} \quad\longrightarrow\quad p_{1} \;-\; \frac{\Delta p + \Delta n}{\rm tn+fn+tp+fp}\\
p_{0} &= \frac{\rm tn+fn}{\rm tn+fn+tp+fp} \quad\longrightarrow\quad p_{0} \;+\; \frac{\Delta p + \Delta n}{\rm tn+fn+tp+fp}
\end{align*}
</script></div>

<p>For the likelihoods:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
S_{e} &= \frac{\rm tp}{\rm tp+fn} \quad\longrightarrow\quad S_{e}\;-\; \frac{\Delta p}{\rm tp + fn}\\
\beta &= \frac{\rm fn}{\rm tp+fn} \quad\longrightarrow\quad \beta\;+\; \frac{\Delta p}{\rm tp + fn}\\
S_{p} &= \frac{\rm tn}{\rm tn+fp} \quad\longrightarrow\quad S_{p}\;+\; \frac{\Delta n}{\rm tn + fp}\\
\alpha &= \frac{\rm fp}{\rm tn+fp} \quad\longrightarrow\quad \alpha\;-\; \frac{\Delta n}{\rm tn + fp}
\end{align*}
</script></div>

<p>Note in particular how the recall <span>‚Äã<script type="math/tex">S_{e}</script></span> and class-1 queue rate <span>‚Äã<script type="math/tex">p_{1}</script></span> <em>always</em> decrease when the threshold is increased, explaining the behavior in the graphs. However the change in predictive values is not linear. For the class-1 precision for example, we have:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathit{ppv} = \frac{\rm tp}{\rm tp+fp} \quad\longrightarrow\quad \frac{ {\rm tp} - \Delta p}{ {\rm tp} - \Delta p + {\rm fp} - \Delta n},
</script></div>
<p>so that <span>‚Äã<script type="math/tex">\mathit{ppv}</script></span> will increase if and only if:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\frac{\Delta p}{\rm tp} \; <\; \frac{\Delta n}{\rm fp},
</script></div>
<p>i.e., if the fractional loss of true positives is smaller than the fractional loss of false positives. This inequality can be violated ‚Äúlocally‚Äù by statistical fluctuations (Case 3 in section <a href="#TrendsAndBaselines">9</a>), or ‚Äúglobally‚Äù, when the class-0 score distribution is on the wrong side of the class-1 score distribution (Case 2 in section <a href="#TrendsAndBaselines">9</a>).</p>
:ET